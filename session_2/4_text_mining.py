
# coding: utf-8

# # python hands-on session

# By: Ties de Kok  
# Version: Python 2.7 (see any notes for Python 3.5)

# 1. handling files
# 2. data handling
# 3. web scraping
# 4. **text mining**
# 5. (interactive) visualisations

# ## Introduction

# In the previous notebooks we have either worked with quantitative data (Pandas) or we have extracted information from webpages or documents.  
# In this notebook we will try to convert qualitative data, more specifically text, into something that we can use for analysis.  
# 
# Extracting this "information" from text is often called "text mining" or "natural language processing" (NLP)
# 
# NLP is a sub-field that can easily fill an entire separate workshop, here I will only touch upon the basics.

# Basic steps for NLP:
# 1. Obtain and load some raw text
# 2. Process this text ("clean" the text)
# 3. Analyse the text

# **Note: For the sake of consistency I have prepared this notebook with Python 2.7**  
# **However, when doing text analysis I highly recommend to use Python 3.5 instead because of improved unicode support!**

# ## Overview of tools

# There are many different "NLP" tools in the Python eco-system.  
# 
# Several well known options:  
# 1. NLTK (Natural Language Toolkit) (http://www.nltk.org/)
# 2. TextBlob (http://textblob.readthedocs.io/en/dev/#)
# 3. Spacy (https://spacy.io/)
# 
# **Installation instructions:**
# 1. NLTK: `pip install nltk` + run `nltk.download()` to download and install the NLTK data
# 2. TextBlob: `pip install -U textblob` + `python -m textblob.download_corpora` in the console to download data
# 3. Spacy: `conda install -c spacy spacy=0.101.0` + `python -m spacy.en.download` in the console to download data
# 

# ## Basic example NLTK

# In[ ]:




# ## Basic example TextBlob

# In[ ]:




# ## Basic example Spacy

# In[ ]:

import spacy


# In[ ]:




# In[ ]:




# In[ ]:




# In[ ]:




# In[ ]:




# In[ ]:




# In[ ]:




# In[ ]:




# In[ ]:




# In[ ]:




# In[ ]:




# In[ ]:



